python -m torch.distributed.launch   --nproc_per_node=2 run_finetune_KT-attn.py --do_train  --do_eval  --fp16  --evaluate_during_training  --task_name  sst2   --model_type roberta   --model_name_or_path roberta-large    --origin_seq_length 128  --max_seq_length  128  --per_gpu_train_batch_size  64   --per_gpu_eval_batch_size 512  --learning_rate 1e-5  --num_train_epochs  3  --max_steps -1   --data_dir ../data/gluedata/SST2/SST2_tagme/SST2_offset  --output_dir AM_15 > log.kt-attn-roberta-base.sst2.64*2.1e-5 
