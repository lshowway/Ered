#### the command of pretraining K-module
nohup python run_pretrain_kformers.py   --do_train  --model_type roberta-base  --model_name_or_path  roberta-base  --data_dir  ../data\knowledge\pretrain\wikidata_description  --max_seq_length 32  --num_neg_sample 10  --entity_emb_size 32  --per_gpu_train_batch_size 128  --per_gpu_eval_batch_size 512  --gradient_accumulation_steps 1  --learning_rate 3e-5  --weight_decay 0.0  --num_train_epochs 3  --max_steps 1  --warmup_steps 1  --save_steps 1  --eval_steps 1  --seed  3407  --fp16