#### the command of pretraining K-module


nohup python -m torch.distributed.launch   --nproc_per_node=4    run_pretrain_kformers.py   --do_train  --model_type roberta  --model_name_or_path  roberta-large  --data_dir  ../knowledge_resource/dbpedia_abstract_corpus  --max_seq_length 128   --entity_emb_size 128  --per_gpu_train_batch_size 16  --gradient_accumulation_steps 1  --learning_rate 1e-5  --weight_decay 0.01  --num_train_epochs 10  --max_steps 1000  --warmup_steps -1  --max_save_checkpoints 20  --save_steps -1  --logging_steps  100  --seed  3407  --fp16  > Kmodule.post-train.msl=128.emb_size=128.lr=1e-5.16*4 &